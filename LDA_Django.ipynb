{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import sklearn\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "             \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \n",
    "             'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "             'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', \n",
    "             'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \n",
    "             'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
    "             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
    "             'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', \n",
    "             'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', \n",
    "             'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \n",
    "             \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \n",
    "             \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \n",
    "             \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n",
    "             'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "             'wouldn', \"wouldn't\",\"say\"]\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "cv= CountVectorizer(lowercase=True, max_df=.90, min_df=3, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['#Agent','#Customer','ameriprise','hi','ok','im','account','client','group','um','umm','unk','sir','maam','zero','one','two','three','four','five','six','seven','eight','nine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript</th>\n",
       "      <th>HDS Call Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Agent : THANK YOU FOR CALLING &lt;UNK&gt; MY NAME I...</td>\n",
       "      <td>1530451759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Agent : BROKERAGE ANY BUSINESS MY NAME IS SHE...</td>\n",
       "      <td>1530451784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Agent : [LAUGHTER] THANK YOU FOR CALLING AMER...</td>\n",
       "      <td>1530451816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Customer : MY SERVICE // #Agent : THANK YOU F...</td>\n",
       "      <td>1530451805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Agent : THANK YOU FOR CALLING OUTGOING TRANSF...</td>\n",
       "      <td>1530451879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Transcript  HDS Call Index\n",
       "0  #Agent : THANK YOU FOR CALLING <UNK> MY NAME I...      1530451759\n",
       "1  #Agent : BROKERAGE ANY BUSINESS MY NAME IS SHE...      1530451784\n",
       "2  #Agent : [LAUGHTER] THANK YOU FOR CALLING AMER...      1530451816\n",
       "3  #Customer : MY SERVICE // #Agent : THANK YOU F...      1530451805\n",
       "4  #Agent : THANK YOU FOR CALLING OUTGOING TRANSF...      1530451879"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base_Data = pd.read_csv(r\"C:\\Users\\aprabh13\\OneDrive - Ameriprise\\NLP\\Speech_Data\\Top_10000.csv\")\n",
    "Base_Data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Agent : THANK YOU FOR CALLING &lt;UNK&gt; MY NAME I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Agent : BROKERAGE ANY BUSINESS MY NAME IS SHE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Agent : [LAUGHTER] THANK YOU FOR CALLING AMER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Customer : MY SERVICE // #Agent : THANK YOU F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Agent : THANK YOU FOR CALLING OUTGOING TRANSF...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data\n",
       "0  #Agent : THANK YOU FOR CALLING <UNK> MY NAME I...\n",
       "1  #Agent : BROKERAGE ANY BUSINESS MY NAME IS SHE...\n",
       "2  #Agent : [LAUGHTER] THANK YOU FOR CALLING AMER...\n",
       "3  #Customer : MY SERVICE // #Agent : THANK YOU F...\n",
       "4  #Agent : THANK YOU FOR CALLING OUTGOING TRANSF..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base_Data=Base_Data[['Transcript']]\n",
    "Base_Data.rename(columns={Base_Data.columns[0]:\"Data\"},inplace=True)\n",
    "Base_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    data =''\n",
    "    for token in tokenizer.tokenize(text):\n",
    "        token=token.lower()\n",
    "        token=WordNetLemmatizer().lemmatize(token,pos='v')\n",
    "        if token not in stopwords :\n",
    "            data=data+(token)+' '\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    tags=nltk.pos_tag(tokenizer.tokenize(text))\n",
    "    noun_verbs = [word for word,pos in tags if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS' or pos == 'VB' or pos == 'VBD' or pos == 'VBG' or pos == 'VBN'or pos == 'VBP' or pos == 'VBZ')]\n",
    "    data=''\n",
    "    for word in noun_verbs:\n",
    "        data=data+word+' '\n",
    "    return data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_prep_LDA(Data,Special_words=\"\",NV_only=0):\n",
    "    global dtm,Data_preprocess\n",
    "    stopwords.extend(Special_words)\n",
    "    Data_wo_punc=Data['Data'].map(remove_punctuations)\n",
    "    Data_preprocess=Data_wo_punc.map(preprocess)\n",
    "    if NV_only==1:\n",
    "        Data_preprocess=Data_preprocess.map(pos_tagging)\n",
    "    dtm = cv.fit_transform(Data_preprocess)\n",
    "    \n",
    "    perplexity_values = []\n",
    "    topic = []\n",
    "\n",
    "    for x in range (10,13):      \n",
    "        topic.append(x)\n",
    "        LDA = LatentDirichletAllocation(n_components=x,random_state=420,doc_topic_prior=0.1,topic_word_prior=0.1)\n",
    "        LDA.fit(dtm)\n",
    "        z=LDA.perplexity(dtm)\n",
    "        perplexity_values.append(z)\n",
    "        \n",
    "    return topic,perplexity_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic,perplexity=Data_prep_LDA(Base_Data,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 12] [640.1283934419564, 639.784189925023, 635.7774856884248]\n"
     ]
    }
   ],
   "source": [
    "print(topic,perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(dtm,n_topics):\n",
    "    global LDA_model,Data_preprocess\n",
    "    LDA_model = LatentDirichletAllocation(n_components=n_topics,random_state=420,doc_topic_prior=0.1,topic_word_prior=0.1,\n",
    "                                      learning_method='online')\n",
    "    LDA_model.fit(dtm)\n",
    "    lda_output = LDA_model.transform(dtm)\n",
    "\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(LDA_model.n_components)]\n",
    "\n",
    "    # index names\n",
    "    docnames = [\"Doc\" + str(i) for i in range(len(Data_preprocess))]\n",
    "\n",
    "    # Make the pandas dataframe\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "    # Get dominant topic for each document\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['dominant_topic'] = dominant_topic\n",
    "    return df_document_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic=LDA(dtm,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "      <th>Topic10</th>\n",
       "      <th>Topic11</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc9995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc9996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc9997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc9998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc9999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Topic0  Topic1  Topic2  Topic3  Topic4  Topic5  Topic6  Topic7  \\\n",
       "Doc0        0.0    0.00     0.0    0.00    0.62    0.00    0.00    0.00   \n",
       "Doc1        0.0    0.02     0.0    0.00    0.23    0.00    0.11    0.50   \n",
       "Doc2        0.0    0.00     0.0    0.23    0.39    0.18    0.00    0.17   \n",
       "Doc3        0.0    0.00     0.0    0.00    0.85    0.00    0.00    0.08   \n",
       "Doc4        0.0    0.24     0.0    0.00    0.39    0.00    0.11    0.00   \n",
       "...         ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "Doc9995     0.0    0.42     0.0    0.00    0.00    0.00    0.06    0.00   \n",
       "Doc9996     0.0    0.00     0.0    0.00    0.00    0.12    0.03    0.04   \n",
       "Doc9997     0.0    0.00     0.0    0.03    0.82    0.00    0.00    0.00   \n",
       "Doc9998     0.0    0.00     0.0    0.10    0.48    0.00    0.00    0.17   \n",
       "Doc9999     0.0    0.02     0.0    0.44    0.13    0.00    0.02    0.10   \n",
       "\n",
       "         Topic8  Topic9  Topic10  Topic11  dominant_topic  \n",
       "Doc0       0.00    0.14     0.23     0.00               4  \n",
       "Doc1       0.00    0.13     0.00     0.00               7  \n",
       "Doc2       0.00    0.00     0.00     0.02               4  \n",
       "Doc3       0.00    0.05     0.00     0.00               4  \n",
       "Doc4       0.00    0.25     0.00     0.00               4  \n",
       "...         ...     ...      ...      ...             ...  \n",
       "Doc9995    0.00    0.50     0.00     0.00               9  \n",
       "Doc9996    0.00    0.80     0.00     0.00               9  \n",
       "Doc9997    0.03    0.12     0.00     0.00               4  \n",
       "Doc9998    0.02    0.17     0.06     0.00               4  \n",
       "Doc9999    0.00    0.19     0.08     0.00               3  \n",
       "\n",
       "[10000 rows x 13 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_words():\n",
    "    for index,topic in enumerate(LDA_model.components_):\n",
    "        print(f'THE TOP 10 WORDS FOR TOPIC #{index}')\n",
    "        print([cv.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 10 WORDS FOR TOPIC #0\n",
      "['clark', 'assurance', 'rebate', 'dispute', 'events', 'tracy', 'reward', 'maryland', 'calendar', 'fail']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #1\n",
      "['beneficiary', 'receive', 'claim', 'thats', 'address', 'know', 'need', 'form', 'letter', 'send']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #2\n",
      "['xray', 'assets', 'heather', 'bruce', 'exclusive', 'melissa', 'diana', 'travel', 'privilege', 'mary']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #3\n",
      "['team', 'sure', 'try', 'like', 'help', 'advisor', 'yeah', 'id', 'service', 'mhm']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #4\n",
      "['right', 'check', 'let', 'need', 'thats', 'information', 'dont', 'mhm', 'yes', 'number']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #5\n",
      "['try', 'youre', 'look', 'gonna', 'dont', 'thats', 'right', 'like', 'yeah', 'know']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #6\n",
      "['come', 'cash', 'broker', 'party', 'fund', 'money', 'company', 'service', 'brokerage', 'transfer']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #7\n",
      "['mhm', 'let', 'need', 'like', 'right', 'look', 'thats', 'yeah', 'form', 'case']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #8\n",
      "['minutes', 'wait', 'representatives', 'busy', 'hear', 'currently', 'available', 'continue', 'hold', 'hello']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #9\n",
      "['help', 'line', 'connect', 'ill', 'service', 'financial', 'insurance', 'policy', 'yes', 'number']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #10\n",
      "['money', 'bank', 'gonna', 'dont', 'look', 'right', 'yeah', 'like', 'thats', 'know']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #11\n",
      "['consolidate', 'look', 'feel', 'future', 'today', 'advisor', 'help', 'retirement', 'financial', 'plan']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_top_words()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
